{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/paddlejob/workspace/work/taozewei/padiff/padiff_env/lib/python3.7/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from padiff import *\n",
    "from padiff import create_model\n",
    "from paddle.io import Dataset, BatchSampler, DataLoader\n",
    "from onnx2pytorch import ConvertModel\n",
    "import onnx\n",
    "import torch\n",
    "import paddle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def traverse_layers_paddle(layer):\n",
    "    for sublayer in layer.sublayers():\n",
    "        print(sublayer.full_name(), sublayer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0627 14:15:47.042629 27237 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 11.8, Runtime API Version: 11.7\n",
      "W0627 14:15:47.043684 27237 gpu_resources.cc:149] device: 0, cuDNN Version: 8.6.\n"
     ]
    }
   ],
   "source": [
    "class SimpleModule(torch.nn.Module):\n",
    "  def __init__(self):\n",
    "    super(SimpleModule, self).__init__()\n",
    "    self.linear1 = torch.nn.Linear(100, 10)\n",
    "  def forward(self, x):\n",
    "    x = self.linear1(x)\n",
    "    return x\n",
    "\n",
    "class SimpleLayer(paddle.nn.Layer):\n",
    "  def __init__(self):\n",
    "    super(SimpleLayer, self).__init__()\n",
    "    self.linear1 = paddle.nn.Linear(100, 10)\n",
    "    # self.linear2 = paddle.nn.Linear(10, 10)\n",
    "  def forward(self, x):\n",
    "    x = self.linear1(x)\n",
    "    # x = self.linear2(x)\n",
    "    return x\n",
    "\n",
    "module = SimpleModule()\n",
    "layer = SimpleLayer()\n",
    "\n",
    "inp = paddle.rand((1, 100, 100)).numpy().astype(\"float32\")\n",
    "inp = ({'x': torch.as_tensor(inp) },\n",
    "     {'x': paddle.to_tensor(inp)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomDataset(Dataset):\n",
    "    def __init__(self, num_samples):\n",
    "        self.num_samples = num_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input = paddle.rand((100, 100))\n",
    "        return input\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "    \n",
    "dataset = RandomDataset(10)\n",
    "\n",
    "loader = DataLoader(dataset,\n",
    "                    batch_size=1,\n",
    "                    shuffle=True,\n",
    "                    drop_last=True,\n",
    "                    num_workers=2)\n",
    "\n",
    "loss_fn = paddle.mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(SimpleLayer())\n",
    "\n",
    "for data in loader():\n",
    "    output = model(data)\n",
    "    loss = loss_fn(output)\n",
    "    model.backward(loss)\n",
    "\n",
    "    model.try_dump(per_step=10)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(SimpleLayer())\n",
    "for i in range(100):\n",
    "    output = model(inp[1]['x']) \n",
    "    model.try_dump(per_step=2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_args = [\"-c\",\n",
    "           \"/root/paddlejob/workspace/work/taozewei/padiff/PaddleFleetX/ppfleetx/configs/nlp/gpt/pretrain_gpt_345M_single_card.yaml\",\n",
    "           \"-o\",\n",
    "           \"Model.fused_softmax_with_triangular=false\",\n",
    "           \"-o\",\n",
    "           \"Model.fuse_attn_qkv=false\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/paddlejob/workspace/work/taozewei/padiff/padiff_env/lib/python3.7/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/root/paddlejob/workspace/work/taozewei/padiff/padiff_env/lib/python3.7/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n",
      "\u001b[32m[2023-06-27 13:10:56,430] [INFO]\u001b[0m - The global seed is set to 2049 and local seed is set to 2050.\u001b[0m\n",
      "\u001b[32m[2023-06-27 13:10:56,431] [INFO]\u001b[0m - Found gpt2-vocab.json in cache_dir: /root/.cache/ppfleetx/.\u001b[0m\n",
      "\u001b[32m[2023-06-27 13:10:56,432] [INFO]\u001b[0m - Found gpt2-merges.txt in cache_dir: /root/.cache/ppfleetx/.\u001b[0m\n",
      "\u001b[32m[2023-06-27 13:10:56,433] [INFO]\u001b[0m - loading vocabulary file http://fleet.bj.bcebos.com/datasets/gpt/gpt2-vocab.json from cache at /root/.cache/ppfleetx/gpt2-vocab.json\u001b[0m\n",
      "\u001b[32m[2023-06-27 13:10:56,433] [INFO]\u001b[0m - loading merges file http://fleet.bj.bcebos.com/datasets/gpt/gpt2-merges.txt from cache at /root/.cache/ppfleetx/gpt2-merges.txt\u001b[0m\n",
      "WARNING:root: > padded vocab (size: 50304) with 0 dummy tokens (new size: 50304)\n",
      "\u001b[32m[2023-06-27 13:10:56,505] [INFO]\u001b[0m - Model Size: 0.35 B\u001b[0m\n",
      "W0627 13:10:56.510839 11400 gpu_resources.cc:119] Please NOTE: device: 0, GPU Compute Capability: 8.0, Driver API Version: 11.8, Runtime API Version: 11.7\n",
      "W0627 13:10:56.511888 11400 gpu_resources.cc:149] device: 0, cuDNN Version: 8.6.\n",
      "\u001b[32m[2023-06-27 13:10:58,328] [INFO]\u001b[0m - \n",
      "===========================================================\n",
      "==       PaddleFleetX is powered by PaddlePaddle !       ==\n",
      "===========================================================\n",
      "==                                                       ==\n",
      "==   For more info please go to the following website.   ==\n",
      "==                                                       ==\n",
      "==      https://github.com/PaddlePaddle/PaddleFleetX     ==\n",
      "===========================================================\n",
      "\u001b[0m\n",
      "\u001b[32m[2023-06-27 13:10:58,328] [INFO]\u001b[0m - Data : \u001b[0m\n",
      "\u001b[32m[2023-06-27 13:10:58,329] [INFO]\u001b[0m -     Eval : \u001b[0m\n",
      "\u001b[32m[2023-06-27 13:10:58,329] [INFO]\u001b[0m -         dataset : \u001b[0m\n",
      "\u001b[32m[2023-06-27 13:10:58,330] [INFO]\u001b[0m -             input_dir : ./data/\u001b[0m\n",
      "\u001b[32m[2023-06-27 13:10:58,330] [INFO]\u001b[0m -             max_seq_len : 1024\u001b[0m\n",
      "\u001b[32m[2023-06-27 13:10:58,330] [INFO]\u001b[0m -             mode : Eval\u001b[0m\n",
      "\u001b[32m[2023-06-27 13:10:58,331] [INFO]\u001b[0m -             model_type : GPT\u001b[0m\n",
      "\u001b[32m[2023-06-27 13:10:58,331] [INFO]\u001b[0m -             name : GPTDataset\u001b[0m\n",
      "\u001b[32m[2023-06-27 13:10:58,332] [INFO]\u001b[0m -             num_samples : 80080\u001b[0m\n",
      "\u001b[32m[2023-06-27 13:10:58,332] [INFO]\u001b[0m -             seed : 1024\u001b[0m\n",
      "\u001b[32m[2023-06-27 13:10:58,332] [INFO]\u001b[0m -             split : [969, 30, 1]\u001b[0m\n",
      "\u001b[32m[2023-06-27 13:10:58,333] [INFO]\u001b[0m -         loader : \u001b[0m\n",
      "\u001b[32m[2023-06-27 13:10:58,333] [INFO]\u001b[0m -             collate_fn : gpt_collate_fn\u001b[0m\n",
      "\u001b[32m[2023-06-27 13:10:58,334] [INFO]\u001b[0m -             num_workers : 1\u001b[0m\n",
      "\u001b[32m[2023-06-27 13:10:58,334] [INFO]\u001b[0m -             return_list : False\u001b[0m\n",
      "\u001b[32m[2023-06-27 13:10:58,336] [INFO]\u001b[0m -         sampler : \u001b[0m\n",
      "\u001b[32m[2023-06-27 13:10:58,336] [INFO]\u001b[0m -             batch_size : 8\u001b[0m\n",
      "\u001b[32m[2023-06-27 13:10:58,336] [INFO]\u001b[0m -             drop_last : True\u001b[0m\n",
      "\u001b[32m[2023-06-27 13:10:58,337] [INFO]\u001b[0m -             name : GPTBatchSampler\u001b[0m\n",
      "\u001b[32m[2023-06-27 13:10:58,337] [INFO]\u001b[0m -             shuffle : False\u001b[0m\n",
      "\u001b[32m[2023-06-27 13:10:58,338] [INFO]\u001b[0m -     Train : \u001b[0m\n",
      "\u001b[32m[2023-06-27 13:10:58,338] [INFO]\u001b[0m -         dataset : \u001b[0m\n",
      "\u001b[32m[2023-06-27 13:10:58,338] [INFO]\u001b[0m -             input_dir : ./data/\u001b[0m\n",
      "\u001b[32m[2023-06-27 13:10:58,339] [INFO]\u001b[0m -             max_seq_len : 1024\u001b[0m\n",
      "\u001b[32m[2023-06-27 13:10:58,339] [INFO]\u001b[0m -             mode : Train\u001b[0m\n",
      "\u001b[32m[2023-06-27 13:10:58,340] [INFO]\u001b[0m -             model_type : GPT\u001b[0m\n",
      "\u001b[32m[2023-06-27 13:10:58,340] [INFO]\u001b[0m -             name : GPTDataset\u001b[0m\n",
      "\u001b[32m[2023-06-27 13:10:58,340] [INFO]\u001b[0m -             num_samples : 4000000\u001b[0m\n",
      "\u001b[32m[2023-06-27 13:10:58,341] [INFO]\u001b[0m -             seed : 1024\u001b[0m\n",
      "\u001b[32m[2023-06-27 13:10:58,341] [INFO]\u001b[0m -             split : [969, 30, 1]\u001b[0m\n",
      "\u001b[32m[2023-06-27 13:10:58,341] [INFO]\u001b[0m -         loader : \u001b[0m\n",
      "\u001b[32m[2023-06-27 13:10:58,342] [INFO]\u001b[0m -             collate_fn : gpt_collate_fn\u001b[0m\n",
      "\u001b[32m[2023-06-27 13:10:58,342] [INFO]\u001b[0m -             num_workers : 1\u001b[0m\n",
      "\u001b[32m[2023-06-27 13:10:58,343] [INFO]\u001b[0m -             return_list : False\u001b[0m\n",
      "\u001b[32m[2023-06-27 13:10:58,343] [INFO]\u001b[0m -         sampler : \u001b[0m\n",
      "\u001b[32m[2023-06-27 13:10:58,343] [INFO]\u001b[0m -             batch_size : 8\u001b[0m\n",
      "\u001b[32m[2023-06-27 13:10:58,346] [INFO]\u001b[0m -             drop_last : True\u001b[0m\n",
      "\u001b[32m[2023-06-27 13:10:58,346] [INFO]\u001b[0m -             name : GPTBatchSampler\u001b[0m\n",
      "\u001b[32m[2023-06-27 13:10:58,346] [INFO]\u001b[0m -             shuffle : False\u001b[0m\n",
      "\u001b[32m[2023-06-27 13:10:58,347] [INFO]\u001b[0m - Distributed : \u001b[0m\n",
      "\u001b[32m[2023-06-27 13:10:58,347] [INFO]\u001b[0m -     dp_degree : 1\u001b[0m\n",
      "\u001b[32m[2023-06-27 13:10:58,348] [INFO]\u001b[0m -     fuse_sequence_parallel_allreduce : False\u001b[0m\n",
      "\u001b[32m[2023-06-27 13:10:58,348] [INFO]\u001b[0m -     hcg : HybridCommunicateGroup\u001b[0m\n",
      "\u001b[32m[2023-06-27 13:10:58,348] [INFO]\u001b[0m -     mp_degree : 1\u001b[0m\n",
      "\u001b[32m[2023-06-27 13:10:58,349] [INFO]\u001b[0m -     pp_degree : 1\u001b[0m\n",
      "\u001b[32m[2023-06-27 13:10:58,349] [INFO]\u001b[0m -     pp_recompute_interval : 1\u001b[0m\n",
      "\u001b[32m[2023-06-27 13:10:58,349] [INFO]\u001b[0m -     sharding : \u001b[0m\n",
      "\u001b[32m[2023-06-27 13:10:58,350] [INFO]\u001b[0m -         broadcast_overlap : False\u001b[0m\n",
      "\u001b[32m[2023-06-27 13:10:58,350] [INFO]\u001b[0m -         reduce_overlap : False\u001b[0m\n",
      "\u001b[32m[2023-06-27 13:10:58,351] [INFO]\u001b[0m -         sharding_degree : 1\u001b[0m\n",
      "\u001b[32m[2023-06-27 13:10:58,351] [INFO]\u001b[0m -         sharding_offload : False\u001b[0m\n",
      "\u001b[32m[2023-06-27 13:10:58,351] [INFO]\u001b[0m -         sharding_stage : 1\u001b[0m\n",
      "\u001b[32m[2023-06-27 13:10:58,352] [INFO]\u001b[0m - Engine : \u001b[0m\n",
      "\u001b[32m[2023-06-27 13:10:58,352] [INFO]\u001b[0m -     accumulate_steps : 1\u001b[0m\n",
      "\u001b[32m[2023-06-27 13:10:58,353] [INFO]\u001b[0m -     eval_freq : 500\u001b[0m\n",
      "\u001b[32m[2023-06-27 13:10:58,353] [INFO]\u001b[0m -     eval_iters : 10\u001b[0m\n",
      "\u001b[32m[2023-06-27 13:10:58,353] [INFO]\u001b[0m -     logging_freq : 1\u001b[0m\n",
      "\u001b[32m[2023-06-27 13:10:58,354] [INFO]\u001b[0m -     max_steps : 500000\u001b[0m\n",
      "\u001b[32m[2023-06-27 13:10:58,354] [INFO]\u001b[0m -     mix_precision : \u001b[0m\n",
      "\u001b[32m[2023-06-27 13:10:58,354] [INFO]\u001b[0m -         custom_black_list : ['reduce_sum', 'c_softmax_with_cross_entropy', 'elementwise_div']\u001b[0m\n",
      "\u001b[32m[2023-06-27 13:10:58,355] [INFO]\u001b[0m -         custom_white_list : ['lookup_table', 'lookup_table_v2']\u001b[0m\n",
      "\u001b[32m[2023-06-27 13:10:58,355] [INFO]\u001b[0m -         dtype : float16\u001b[0m\n",
      "\u001b[32m[2023-06-27 13:10:58,356] [INFO]\u001b[0m -         enable : True\u001b[0m\n",
      "\u001b[32m[2023-06-27 13:10:58,356] [INFO]\u001b[0m -         level : O2\u001b[0m\n",
      "\u001b[32m[2023-06-27 13:10:58,356] [INFO]\u001b[0m -         scale_loss : 32768.0\u001b[0m\n",
      "\u001b[32m[2023-06-27 13:10:58,357] [INFO]\u001b[0m -     num_train_epochs : 1\u001b[0m\n",
      "\u001b[32m[2023-06-27 13:10:58,357] [INFO]\u001b[0m -     save_load : \u001b[0m\n",
      "\u001b[32m[2023-06-27 13:10:58,358] [INFO]\u001b[0m -         ckpt_dir : None\u001b[0m\n",
      "\u001b[32m[2023-06-27 13:10:58,358] [INFO]\u001b[0m -         output_dir : ./output\u001b[0m\n",
      "\u001b[32m[2023-06-27 13:10:58,358] [INFO]\u001b[0m -         save_epoch : 1\u001b[0m\n",
      "\u001b[32m[2023-06-27 13:10:58,359] [INFO]\u001b[0m -         save_steps : 1000\u001b[0m\n",
      "\u001b[32m[2023-06-27 13:10:58,359] [INFO]\u001b[0m -     test_iters : 100\u001b[0m\n",
      "\u001b[32m[2023-06-27 13:10:58,359] [INFO]\u001b[0m - Global : \u001b[0m\n",
      "\u001b[32m[2023-06-27 13:10:58,360] [INFO]\u001b[0m -     device : gpu\u001b[0m\n",
      "\u001b[32m[2023-06-27 13:10:58,360] [INFO]\u001b[0m -     enable_partial_send_recv : True\u001b[0m\n",
      "\u001b[32m[2023-06-27 13:10:58,361] [INFO]\u001b[0m -     global_batch_size : 8\u001b[0m\n",
      "\u001b[32m[2023-06-27 13:10:58,361] [INFO]\u001b[0m -     local_batch_size : 8\u001b[0m\n",
      "\u001b[32m[2023-06-27 13:10:58,361] [INFO]\u001b[0m -     micro_batch_size : 8\u001b[0m\n",
      "\u001b[32m[2023-06-27 13:10:58,362] [INFO]\u001b[0m -     seed : 1024\u001b[0m\n",
      "\u001b[32m[2023-06-27 13:10:58,362] [INFO]\u001b[0m - Model : \u001b[0m\n",
      "\u001b[32m[2023-06-27 13:10:58,363] [INFO]\u001b[0m -     attention_probs_dropout_prob : 0.1\u001b[0m\n",
      "\u001b[32m[2023-06-27 13:10:58,363] [INFO]\u001b[0m -     ffn_hidden_size : 4096\u001b[0m\n",
      "\u001b[32m[2023-06-27 13:10:58,363] [INFO]\u001b[0m -     fuse_attn_qkv : false\u001b[0m\n",
      "\u001b[32m[2023-06-27 13:10:58,364] [INFO]\u001b[0m -     fused_linear : False\u001b[0m\n",
      "\u001b[32m[2023-06-27 13:10:58,364] [INFO]\u001b[0m -     fused_softmax_with_triangular : false\u001b[0m\n",
      "\u001b[32m[2023-06-27 13:10:58,364] [INFO]\u001b[0m -     hidden_dropout_prob : 0.1\u001b[0m\n",
      "\u001b[32m[2023-06-27 13:10:58,365] [INFO]\u001b[0m -     hidden_size : 1024\u001b[0m\n",
      "\u001b[32m[2023-06-27 13:10:58,365] [INFO]\u001b[0m -     initializer_range : 0.02\u001b[0m\n",
      "\u001b[32m[2023-06-27 13:10:58,366] [INFO]\u001b[0m -     max_position_embeddings : 1024\u001b[0m\n",
      "\u001b[32m[2023-06-27 13:10:58,366] [INFO]\u001b[0m -     module : GPTModule\u001b[0m\n",
      "\u001b[32m[2023-06-27 13:10:58,366] [INFO]\u001b[0m -     name : GPT\u001b[0m\n",
      "\u001b[32m[2023-06-27 13:10:58,367] [INFO]\u001b[0m -     no_recompute_layers : None\u001b[0m\n",
      "\u001b[32m[2023-06-27 13:10:58,367] [INFO]\u001b[0m -     num_attention_heads : 16\u001b[0m\n",
      "\u001b[32m[2023-06-27 13:10:58,368] [INFO]\u001b[0m -     num_layers : 24\u001b[0m\n",
      "\u001b[32m[2023-06-27 13:10:58,368] [INFO]\u001b[0m -     recompute_granularity : None\u001b[0m\n",
      "\u001b[32m[2023-06-27 13:10:58,368] [INFO]\u001b[0m -     scale_qk_by_layer_num : True\u001b[0m\n",
      "\u001b[32m[2023-06-27 13:10:58,369] [INFO]\u001b[0m -     sequence_parallel : False\u001b[0m\n",
      "\u001b[32m[2023-06-27 13:10:58,369] [INFO]\u001b[0m -     type_vocab_size : 16\u001b[0m\n",
      "\u001b[32m[2023-06-27 13:10:58,369] [INFO]\u001b[0m -     use_flash_attn : False\u001b[0m\n",
      "\u001b[32m[2023-06-27 13:10:58,370] [INFO]\u001b[0m -     use_recompute : False\u001b[0m\n",
      "\u001b[32m[2023-06-27 13:10:58,370] [INFO]\u001b[0m -     vocab_size : 50304\u001b[0m\n",
      "\u001b[32m[2023-06-27 13:10:58,371] [INFO]\u001b[0m -     vocab_size_divisible_unit : 128\u001b[0m\n",
      "\u001b[32m[2023-06-27 13:10:58,371] [INFO]\u001b[0m - Optimizer : \u001b[0m\n",
      "\u001b[32m[2023-06-27 13:10:58,371] [INFO]\u001b[0m -     beta1 : 0.9\u001b[0m\n",
      "\u001b[32m[2023-06-27 13:10:58,372] [INFO]\u001b[0m -     beta2 : 0.999\u001b[0m\n",
      "\u001b[32m[2023-06-27 13:10:58,372] [INFO]\u001b[0m -     epsilon : 1e-08\u001b[0m\n",
      "\u001b[32m[2023-06-27 13:10:58,373] [INFO]\u001b[0m -     grad_clip : \u001b[0m\n",
      "\u001b[32m[2023-06-27 13:10:58,373] [INFO]\u001b[0m -         clip_norm : 1.0\u001b[0m\n",
      "\u001b[32m[2023-06-27 13:10:58,373] [INFO]\u001b[0m -         name : ClipGradByGlobalNorm\u001b[0m\n",
      "\u001b[32m[2023-06-27 13:10:58,374] [INFO]\u001b[0m -     lr : \u001b[0m\n",
      "\u001b[32m[2023-06-27 13:10:58,374] [INFO]\u001b[0m -         decay_steps : 2880000\u001b[0m\n",
      "\u001b[32m[2023-06-27 13:10:58,374] [INFO]\u001b[0m -         max_lr : 5e-05\u001b[0m\n",
      "\u001b[32m[2023-06-27 13:10:58,375] [INFO]\u001b[0m -         min_lr : 1e-05\u001b[0m\n",
      "\u001b[32m[2023-06-27 13:10:58,375] [INFO]\u001b[0m -         name : CosineAnnealingWithWarmupDecay\u001b[0m\n",
      "\u001b[32m[2023-06-27 13:10:58,376] [INFO]\u001b[0m -         use_increments : True\u001b[0m\n",
      "\u001b[32m[2023-06-27 13:10:58,376] [INFO]\u001b[0m -         warmup_rate : 0.01\u001b[0m\n",
      "\u001b[32m[2023-06-27 13:10:58,376] [INFO]\u001b[0m -     multi_precision : True\u001b[0m\n",
      "\u001b[32m[2023-06-27 13:10:58,377] [INFO]\u001b[0m -     name : FusedAdamW\u001b[0m\n",
      "\u001b[32m[2023-06-27 13:10:58,377] [INFO]\u001b[0m -     tensor_fusion : False\u001b[0m\n",
      "\u001b[32m[2023-06-27 13:10:58,378] [INFO]\u001b[0m -     weight_decay : 0.01\u001b[0m\n",
      "\u001b[32m[2023-06-27 13:10:58,378] [INFO]\u001b[0m - Profiler : \u001b[0m\n",
      "\u001b[32m[2023-06-27 13:10:58,378] [INFO]\u001b[0m -     detailed : False\u001b[0m\n",
      "\u001b[32m[2023-06-27 13:10:58,379] [INFO]\u001b[0m -     enable : False\u001b[0m\n",
      "\u001b[32m[2023-06-27 13:10:58,379] [INFO]\u001b[0m -     profiler_log : profiler_log\u001b[0m\n",
      "\u001b[32m[2023-06-27 13:10:58,380] [INFO]\u001b[0m -     scheduler : [1, 5]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from padiff import *\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import copy\n",
    "\n",
    "import paddle\n",
    "from paddle.distributed import fleet\n",
    "import paddle.distributed as dist\n",
    "\n",
    "# __dir__ = os.path.dirname(os.path.abspath(__file__))\n",
    "# sys.path.append(os.path.abspath(os.path.join(__dir__, '../')))\n",
    "\n",
    "from ppfleetx.utils import config\n",
    "from ppfleetx.utils.log import logger\n",
    "from ppfleetx.data import build_dataloader\n",
    "from ppfleetx.models import build_module\n",
    "from ppfleetx.core import EagerEngine\n",
    "from ppfleetx.distributed.apis import env\n",
    "\n",
    "def set_default_flags(flags):\n",
    "    for flag_name, flag_value in flags.items():\n",
    "        if os.getenv(flag_name) is None:\n",
    "            paddle.set_flags({flag_name: flag_value})\n",
    "\n",
    "args = config.parse_args(my_args)\n",
    "cfg = config.get_config(args.config, overrides=args.override, show=False)\n",
    "\n",
    "paddle.set_device(cfg[\"Global\"][\"device\"])\n",
    "if dist.get_world_size() > 1:\n",
    "    env.init_dist_env(cfg)\n",
    "\n",
    "env.set_seed(cfg.Global.seed)\n",
    "\n",
    "module = build_module(cfg)\n",
    "config.print_config(cfg)\n",
    "\n",
    "input = paddle.randint(0, 10000, (1, 1024))\n",
    "\n",
    "my_345m_model = module.model\n",
    "out = my_345m_model(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(my_345m_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    out = model(input)\n",
    "    out.backward()\n",
    "    model.try_dump(per_step=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import urllib.request\n",
    "import re\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "def download(url, file=None):\n",
    "    file = file if file else url.split('/')[-1]\n",
    "    path = os.path.join(os.path.dirname(\"/root/paddlejob/workspace/work/taozewei/padiff/test.ipynb\"), file)\n",
    "    if not os.path.isfile(path):\n",
    "        def reporthook(count, block_size, total_size):\n",
    "            percent = str(int(100 * count * block_size / total_size)) + '%'\n",
    "            print('\\r\\033[KDownloading ' + file + ' (' + percent + ')', end='', flush=True)\n",
    "        urllib.request.urlretrieve(url, path, reporthook=reporthook)\n",
    "        print('\\r\\033[K', end='', flush=True)\n",
    "    return path\n",
    "\n",
    "class BPETokenizer:\n",
    "\n",
    "    def __init__(self):\n",
    "        url = 'https://openaipublic.blob.core.windows.net/gpt-2/models/124M/'\n",
    "        with open(download(url + 'encoder.json'), 'r', encoding='utf-8') as file:\n",
    "            self.encoder = json.load(file)\n",
    "        self.decoder = {v:k for k,v in self.encoder.items()}\n",
    "        with open(download(url + 'vocab.bpe'), 'r', encoding='utf-8') as file:\n",
    "            vocab = file.read().split('\\n')[1:-1]\n",
    "        self.bpe_ranks = {tuple(line.split()): i for i, line in enumerate(vocab)}\n",
    "        assert len(self.encoder) == 50257 and len(self.bpe_ranks) == 50000\n",
    "        bs = list(range(33, 127)) + list(range(161, 256))\n",
    "        xs = list(range(0, 33)) + list(range(127, 161))\n",
    "        cs = bs[:] + [2**8 + i for i in range(len(xs))]\n",
    "        self.byte_encoder = dict(zip(bs + xs, [chr(n) for n in cs]))\n",
    "        self.byte_decoder = {v:k for k, v in self.byte_encoder.items()}\n",
    "\n",
    "    def encode(self, text, allowed_special=None):\n",
    "        tokens = re.findall(r\"\"\"<\\|endoftext\\|>|'s|'t|'re|'ve|'m|'ll|'d| ?\"\"\" +\n",
    "                            r\"\"\"\\w+| ?\\d+| ?[^\\s\\w\\d]+|\\s+(?!\\S)|\\s+\"\"\", text, re.UNICODE)\n",
    "        def translate(token):\n",
    "            if token == '<|endoftext|>':\n",
    "                assert allowed_special and token in allowed_special\n",
    "                return [token]\n",
    "            word = tuple(''.join(self.byte_encoder[byte] for byte in token.encode('utf-8')))\n",
    "            while len(word) != 1:\n",
    "                pairs = set((word[i], word[i+1]) for i in range(len(word)-1))\n",
    "                bigram = min(pairs, key = lambda pair: self.bpe_ranks.get(pair, float('inf')))\n",
    "                if bigram not in self.bpe_ranks:\n",
    "                    break\n",
    "                a, b = bigram\n",
    "                new_word = []\n",
    "                i = 0\n",
    "                while i < len(word):\n",
    "                    j = word.index(a, i) if a in word[i:] else len(word)\n",
    "                    new_word.extend(word[i:j])\n",
    "                    i = j\n",
    "                    if i < len(word):\n",
    "                        j = 2 if i < len(word)-1 and word[i] == a and word[i+1] == b else 1\n",
    "                        new_word.append(a+b if j == 2 else word[i])\n",
    "                        i += j\n",
    "                word = tuple(new_word)\n",
    "            return word\n",
    "        return [self.encoder[_] for token in tokens for _ in translate(token)]\n",
    "\n",
    "    def decode(self, tokens):\n",
    "        tokens = [self.decoder[token] for token in tokens]\n",
    "        buffer = bytearray([self.byte_decoder[c] for c in ''.join(tokens)])\n",
    "        return buffer.decode('utf-8', errors='replace')\n",
    "\n",
    "class GPT2Config:\n",
    "\n",
    "    def __init__(self, model_type):\n",
    "        configs = {\n",
    "            'gpt2':        [ 12, 12, 768  ], # 124M params\n",
    "            'gpt2-medium': [ 24, 16, 1024 ], # 350M params\n",
    "            'gpt2-large':  [ 36, 20, 1280 ], # 774M params\n",
    "            'gpt2-xl':     [ 48, 25, 1600 ]  # 1558M params\n",
    "        }\n",
    "        self.type = model_type\n",
    "        self.n_layer = configs[model_type][0]\n",
    "        self.n_head = configs[model_type][1]\n",
    "        self.n_embd = configs[model_type][2]\n",
    "        self.vocab_size = 50257\n",
    "        self.block_size = 1024\n",
    "        self.url = 'https://huggingface.co/' + model_type + '/resolve/main/pytorch_model.bin'\n",
    "\n",
    "class Attention(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_attn = torch.nn.Linear(config.n_embd, 3 * config.n_embd)\n",
    "        self.c_proj = torch.nn.Linear(config.n_embd, config.n_embd)\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        size = config.block_size\n",
    "        self.register_buffer('bias', torch.tril(torch.ones(size, size)).view(1, 1, size, size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size() # batch, context, embedding\n",
    "        q, k, v  = self.c_attn(x).split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "        att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
    "        att = torch.nn.functional.softmax(att, dim=-1)\n",
    "        return self.c_proj((att @ v).transpose(1, 2).contiguous().view(B, T, C))\n",
    "\n",
    "class Block(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = torch.nn.LayerNorm(config.n_embd)\n",
    "        self.ln_2 = torch.nn.LayerNorm(config.n_embd)\n",
    "        self.attn = Attention(config)\n",
    "        self.mlp = torch.nn.Sequential(collections.OrderedDict([\n",
    "            ('c_fc', torch.nn.Linear(config.n_embd, 4 * config.n_embd)),\n",
    "            ('act', torch.nn.GELU(approximate='tanh')),\n",
    "            ('c_proj', torch.nn.Linear(4 * config.n_embd, config.n_embd))\n",
    "        ]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "\n",
    "class GPT2(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, model_type):\n",
    "        super().__init__()\n",
    "        config = GPT2Config(model_type)\n",
    "        self.block_size = config.block_size\n",
    "        self.transformer = torch.nn.ModuleDict(dict(\n",
    "            wte = torch.nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            wpe = torch.nn.Embedding(config.block_size, config.n_embd),\n",
    "            h = torch.nn.Sequential(*[Block(config) for _ in range(config.n_layer)]),\n",
    "            ln_f = torch.nn.LayerNorm(config.n_embd)\n",
    "        ))\n",
    "        self.lm_head = torch.nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        self.transformer.wte.weight = self.lm_head.weight\n",
    "        # state_file = download(config.url, config.type + '.bin')\n",
    "        # state_dict = torch.load(state_file)\n",
    "        # transposed = [ '.c_attn.weight', '.c_fc.weight', '.c_proj.weight' ]\n",
    "        # for key, value in state_dict.items():\n",
    "        #     if any(key.endswith(w) for w in transposed):\n",
    "        #         state_dict[key] = value.t()\n",
    "        # self.transformer.load_state_dict(state_dict)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, temperature: float = 0.1, top_k: int = 40):\n",
    "        # x = torch.narrow(x, 1, 0, min(x.size(1), self.block_size))\n",
    "        # pos = torch.arange(x.size()[1], dtype=torch.long, device=x.device).unsqueeze(0)\n",
    "        # pos_embbeding = self.transformer.wpe(pos)\n",
    "        x = self.transformer.wte(x)\n",
    "        x = self.lm_head(self.transformer.ln_f(self.transformer.h(x)))\n",
    "        return x\n",
    "        logits = torch.select(x, dim=1, index=-1) / temperature\n",
    "        min_top_k = torch.topk(logits, min(top_k, logits.size(-1))).values[:, [-1]]\n",
    "        logits = torch.where(logits >= min_top_k, logits, -float('Inf'))\n",
    "        probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "        return torch.multinomial(probs, num_samples=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_345m_model = GPT2(\"gpt2-medium\").cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.randint(0, 10000, (1, 1024)).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1024, 50257])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_345m_model(input).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class base:\n",
    "    def __init__(self) -> None:\n",
    "        self.a = 1\n",
    "        self.b = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class son1(base):\n",
    "    pass\n",
    "\n",
    "class son2(base):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = son1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.a = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = son2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "padiff_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
